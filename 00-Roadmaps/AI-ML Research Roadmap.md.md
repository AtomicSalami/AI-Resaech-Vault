## Learning roadmap

### Foundations

- [1986] Backpropagation – Rumelhart et al.
    
    - Link: [https://www.nature.com/articles/323533a0](obsidian://open?vault=ZorDann&file=remote%2F02-Files%2F00-PDFs%2FBackpropagation.pdf)
        
    - Tags: #foundations #nn #backpropagation #gradient-descent #chain-rule
        
- [1998] CNNs – LeCun et al.
    
    - Link: http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf
        
    - Tags: #vision #cnn #nn #feature-extraction #weight-sharing #convolution
        

---

### Sequence Modeling

- [1997] LSTM – Hochreiter & Schmidhuber
    
    - Link: https://www.bioinf.jku.at/publications/older/2604.pdf
        
    - Tags: #sequence-models #lstm #gated-networks #vanishing-gradient
        
- [2013] Word2Vec – Mikolov et al.
    
    - Link: [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)
        
    - Tags: #nlp #word-embeddings #skipgram #cbow #distributional-semantics
        
- [2014] Seq2Seq – Sutskever et al.
    
    - Link: [https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)
        
    - Tags: #sequence-to-sequence #encoder-decoder #nlp #translation
        
- [2015] Bahdanau Attention – Bahdanau et al.
    
    - Link: [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)
        
    - Tags: #attention #nlp #alignment #seq2seq
        
- [2017] Transformers – Vaswani et al.
    
    - Link: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
        
    - Tags: #transformers #attention #self-attention #nlp #deep-learning
        
- [2018] BERT – Devlin et al.
    
    - Link: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
        
    - Tags: #nlp #transformers #bert #masked-language-modeling #transfer-learning
        
- [2018] GPT-1 – Radford et al.
    
    - Link: [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
        
    - Tags: #nlp #transformers #gpt #autoregressive #language-modeling
        
- [2020] Scaling Laws – Kaplan et al.
    
    - Link: [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)
        
    - Tags: #scaling-laws #deep-learning #model-size #compute-efficiency
        
- [2022] InstructGPT (RLHF) – Ouyang et al.
    
    - Link: [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)
        
    - Tags: #alignment #rlhf #reinforcement-learning #human-feedback
        

---

### Vision

- [2015] ResNet – He et al.
    
    - Link: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
        
    - Tags: #vision #cnn #resnet #skip-connections
        
- [2020] Vision Transformers (ViT) – Dosovitskiy et al.
    
    - Link: [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
        
    - Tags: #vision #transformers #vit #image-patches
        

---

### Generative Models

- [2014] GANs – Goodfellow et al.
    
    - Link: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
        
    - Tags: #generative #gans #adversarial-training
        
- [2017] WGAN – Arjovsky et al.
    
    - Link: [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)
        
    - Tags: #gans #wasserstein-distance #stability
        
- [2020] Diffusion Models – Ho et al.
    
    - Link: [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)
        
    - Tags: #generative #diffusion-models #stochastic-processes #denoising